{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b2796e-c98a-4115-8e57-c4ae093c5880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Dataset Files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define dataset paths\n",
    "data_path = \"../data/raw/\"\n",
    "train_dir = os.path.join(data_path, 'train', 'train')\n",
    "test_dir = os.path.join(data_path, 'test', 'test1')\n",
    "\n",
    "# Extract train.zip\n",
    "with zipfile.ZipFile(os.path.join(data_path, 'train.zip'), 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(data_path, 'train'))\n",
    "\n",
    "# Extract test1.zip\n",
    "with zipfile.ZipFile(os.path.join(data_path, 'test1.zip'), 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(data_path, 'test'))\n",
    "\n",
    "# Check extracted files count\n",
    "train_files = os.listdir(os.path.join(data_path, 'train', 'train'))\n",
    "test_files = os.listdir(os.path.join(data_path, 'test', 'test1'))\n",
    "print(f'Train files: {len(train_files)}')\n",
    "print(f'Test files: {len(test_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b25ea2-cf53-4800-b0fb-9e16869eb246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Label Distribution\n",
    "# Count dog and cat images\n",
    "dog_images = [f for f in train_files if 'dog' in f]\n",
    "cat_images = [f for f in train_files if 'cat' in f]\n",
    "\n",
    "print(f\"Number of dog images: {len(dog_images)}\")\n",
    "print(f\"Number of cat images: {len(cat_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db3a26a-f8d2-4486-8007-aae1a15b3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Image Size\n",
    "from PIL import Image\n",
    "\n",
    "# Load and check the size of an image\n",
    "image_path = os.path.join(train_dir, dog_images[0])  # Example: a dog image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "print(f\"Image size: {image.size}\")  # (width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af2a55-35ae-46a6-9462-2027512ead10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation Preview\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load a sample image\n",
    "sample_image = Image.open(image_path)\n",
    "sample_image = sample_image.resize((224, 224))\n",
    "sample_image_array = np.array(sample_image)\n",
    "\n",
    "# Apply augmentation\n",
    "augmented_image = datagen.random_transform(sample_image_array)\n",
    "\n",
    "# Display augmented image\n",
    "plt.imshow(augmented_image)\n",
    "plt.title(\"Augmented Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75904dc1-700e-43db-b5e7-632866756b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing Function\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = img_array / 255.0  # Normalize pixel values (0-255 -> 0-1)\n",
    "    return img_array\n",
    "\n",
    "# Get list of train files\n",
    "train_folder_path = os.path.join('../data/raw/train/train')\n",
    "train_files = os.listdir(train_folder_path)\n",
    "\n",
    "# Load and check first 5 images\n",
    "example_images = train_files[:5]\n",
    "processed_images = [load_and_preprocess_image(os.path.join(train_folder_path, img)) for img in example_images]\n",
    "\n",
    "# Print processed image shapes\n",
    "processed_images_shapes = [img.shape for img in processed_images]\n",
    "print(f\"Processed image shapes: {processed_images_shapes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b1d40-a314-44d7-b908-180f7e462989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the path for the raw training data\n",
    "data_path = \"../data/raw/train/train\"  # Training data path\n",
    "processed_path = \"../data/processed\"   # Directory where organized data will be stored\n",
    "\n",
    "# Create separate directories for cats and dogs under processed folder\n",
    "cats_dir = os.path.join(processed_path, 'cats')\n",
    "dogs_dir = os.path.join(processed_path, 'dogs')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(cats_dir, exist_ok=True)\n",
    "os.makedirs(dogs_dir, exist_ok=True)\n",
    "\n",
    "# Move images to corresponding folders based on names\n",
    "for image_name in os.listdir(data_path):\n",
    "    image_path = os.path.join(data_path, image_name)\n",
    "\n",
    "    if os.path.isfile(image_path):\n",
    "        if 'cat' in image_name.lower():\n",
    "            target_path = os.path.join(cats_dir, image_name)\n",
    "        elif 'dog' in image_name.lower():\n",
    "            target_path = os.path.join(dogs_dir, image_name)\n",
    "        else:\n",
    "            continue  # Skip irrelevant files\n",
    "\n",
    "        shutil.copy(image_path, target_path)\n",
    "\n",
    "print(\"Images successfully organized into cats and dogs directories!\")\n",
    "\n",
    "# Define augmentation settings for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Set the path for the newly organized data (cats and dogs directories)\n",
    "train_dir = os.path.join(processed_path)\n",
    "\n",
    "# Create training data generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  # Training data path\n",
    "    target_size=(224, 224),  # Resize all images to 224x224\n",
    "    batch_size=32,\n",
    "    class_mode='binary',  # Binary classification (dog vs cat)\n",
    "    shuffle=True  # Shuffle data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da10565-97fd-4bbe-9ba2-86f496933a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Updated Data Directories for Cats and Dogs\n",
    "train_dir = \"../data/processed/\"\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Image Preprocessing (ImageDataGenerator)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  \n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # 20% validation data\n",
    ")\n",
    "\n",
    "# Training Dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,  # Reduced for memory optimization\n",
    "    class_mode='binary',\n",
    "    subset=\"training\"  # Using training data subset\n",
    ")\n",
    "\n",
    "# Validation Dataset\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset=\"validation\"  # Using validation data subset\n",
    ")\n",
    "\n",
    "# Model Definition\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional Layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))  \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))  \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten Layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully Connected (Dense) Layer\n",
    "model.add(Dense(128, activation='relu'))  # Reduced neurons from 256 to 128\n",
    "model.add(Dropout(0.5))  # To prevent overfitting\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Save the Model\n",
    "tf.keras.models.save_model(model, '../models/cat_dog_classifier_model.h5')    # Saving the trained model\n",
    "\n",
    "print(\"Model successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc0e19-24eb-4de7-af6e-359956a3dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('../models/cat_dog_classifier_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51626be-3959-4de4-abbd-96b56d644410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Create the results directory if it doesn't exist\n",
    "results_dir = '../results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Create a file to save training and validation accuracy and loss values\n",
    "history_data = history.history\n",
    "\n",
    "# Saving the model summary to a file\n",
    "model_summary_path = os.path.join(results_dir, 'model_summary.txt')\n",
    "with open(model_summary_path, 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Saving training and validation accuracy and loss values to a CSV file\n",
    "history_path = os.path.join(results_dir, 'training_history.csv')\n",
    "with open(history_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['epoch', 'train_loss', 'train_accuracy', 'val_loss', 'val_accuracy'])\n",
    "\n",
    "    # Write the data\n",
    "    for epoch in range(len(history_data['loss'])):\n",
    "        writer.writerow([\n",
    "            epoch + 1,\n",
    "            history_data['loss'][epoch],\n",
    "            history_data['accuracy'][epoch],\n",
    "            history_data['val_loss'][epoch],\n",
    "            history_data['val_accuracy'][epoch]\n",
    "        ])\n",
    "\n",
    "print(\"Model summary and training history have been saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6621e7c-ecd1-469d-b832-b556ec26d6d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm  # To add a progress bar\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('../models/cat_dog_classifier_model.h5')\n",
    "\n",
    "# Test directory path\n",
    "test_dir = \"../data/raw/test/test1\"\n",
    "\n",
    "# Sample submission template\n",
    "sample_submission_path = \"../data/raw/sampleSubmission.csv\"\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "\n",
    "# Initialize an empty list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Get the total number of images in the test directory\n",
    "total_images = len([image_name for image_name in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, image_name))])\n",
    "\n",
    "# Process test images with a custom progress bar\n",
    "for idx, image_name in enumerate(tqdm(os.listdir(test_dir), desc=\"Processing\", total=total_images, unit=\"image\")):\n",
    "    image_path = os.path.join(test_dir, image_name)\n",
    "    \n",
    "    if os.path.isfile(image_path):\n",
    "        # Preprocess the image (resize and normalize)\n",
    "        img = image.load_img(image_path, target_size=(224, 224))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "        img_array /= 255.0  # Normalize the image (same as in training)\n",
    "\n",
    "        # Make prediction (0 for cat, 1 for dog)\n",
    "        prediction = model.predict(img_array, verbose=0)  # Suppress individual prediction output\n",
    "        predicted_class = 1 if prediction[0][0] > 0.5 else 0  # 0 is for cat, 1 is for dog\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "# Add predictions to the sample submission DataFrame with 'id' and 'label' columns\n",
    "submission = pd.DataFrame({\n",
    "    'id': [os.path.splitext(image_name)[0] for image_name in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, image_name))],\n",
    "    'label': predictions\n",
    "})\n",
    "\n",
    "# Save the result as a CSV file in the submission directory\n",
    "submission_path = \"../submission/test_predictions.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
